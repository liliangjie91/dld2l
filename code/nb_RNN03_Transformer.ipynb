{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2228bc-24d1-4e7a-a793-4885b5774398",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T04:12:27.843960Z",
     "iopub.status.busy": "2024-11-07T04:12:27.843960Z",
     "iopub.status.idle": "2024-11-07T04:12:27.851913Z",
     "shell.execute_reply": "2024-11-07T04:12:27.851913Z",
     "shell.execute_reply.started": "2024-11-07T04:12:27.843960Z"
    }
   },
   "outputs": [],
   "source": [
    "import py_mydata_RNN as mrdata\n",
    "import py_mymodel_RNN as mrmodel\n",
    "import py_mypipline_RNN as mrpipl\n",
    "import py_mypipline_S2S as mspipl\n",
    "import torch,math,collections\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l\n",
    "import os,logging\n",
    "# import matplotlib.pyplot as plt\n",
    "LOG_FORMAT = \"%(asctime)s-%(levelname)s-%(message)s\"\n",
    "logging.basicConfig(filename='./log/log.log', level=logging.INFO, format=LOG_FORMAT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed94cd18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[2, 2, 2, 2, 2],\n",
      "         [2, 2, 2, 2, 2]],\n",
      "\n",
      "        [[3, 3, 3, 3, 3],\n",
      "         [3, 3, 3, 3, 3]],\n",
      "\n",
      "        [[4, 4, 4, 4, 4],\n",
      "         [4, 4, 4, 4, 4]]])\n",
      "tensor([[[0, 1, 2, 3, 4],\n",
      "         [0, 1, 2, 3, 4]],\n",
      "\n",
      "        [[0, 1, 2, 3, 4],\n",
      "         [0, 1, 2, 3, 4]],\n",
      "\n",
      "        [[0, 1, 2, 3, 4],\n",
      "         [0, 1, 2, 3, 4]]])\n",
      "tensor([[[False, False,  True,  True,  True],\n",
      "         [False, False,  True,  True,  True]],\n",
      "\n",
      "        [[False, False, False,  True,  True],\n",
      "         [False, False, False,  True,  True]],\n",
      "\n",
      "        [[False, False, False, False,  True],\n",
      "         [False, False, False, False,  True]]])\n"
     ]
    }
   ],
   "source": [
    "valid_lens = torch.tensor([2,3,4])\n",
    "batch_size = len(valid_lens)\n",
    "num_heads,num_layers = 2,2\n",
    "seq_len = 5\n",
    "device = 'cpu'\n",
    "mat_valid = valid_lens.repeat(seq_len,num_layers,1).permute(2,1,0)\n",
    "mat_arrage = torch.arange(seq_len).repeat(batch_size,num_layers,1).to(device)\n",
    "mask = mat_arrage>=mat_valid\n",
    "print(mat_valid)\n",
    "print(mat_arrage)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac5896e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[2, 2, 2, 2, 2],\n",
      "          [2, 2, 2, 2, 2],\n",
      "          [2, 2, 2, 2, 2],\n",
      "          [2, 2, 2, 2, 2],\n",
      "          [2, 2, 2, 2, 2]],\n",
      "\n",
      "         [[2, 2, 2, 2, 2],\n",
      "          [2, 2, 2, 2, 2],\n",
      "          [2, 2, 2, 2, 2],\n",
      "          [2, 2, 2, 2, 2],\n",
      "          [2, 2, 2, 2, 2]]],\n",
      "\n",
      "\n",
      "        [[[3, 3, 3, 3, 3],\n",
      "          [3, 3, 3, 3, 3],\n",
      "          [3, 3, 3, 3, 3],\n",
      "          [3, 3, 3, 3, 3],\n",
      "          [3, 3, 3, 3, 3]],\n",
      "\n",
      "         [[3, 3, 3, 3, 3],\n",
      "          [3, 3, 3, 3, 3],\n",
      "          [3, 3, 3, 3, 3],\n",
      "          [3, 3, 3, 3, 3],\n",
      "          [3, 3, 3, 3, 3]]],\n",
      "\n",
      "\n",
      "        [[[4, 4, 4, 4, 4],\n",
      "          [4, 4, 4, 4, 4],\n",
      "          [4, 4, 4, 4, 4],\n",
      "          [4, 4, 4, 4, 4],\n",
      "          [4, 4, 4, 4, 4]],\n",
      "\n",
      "         [[4, 4, 4, 4, 4],\n",
      "          [4, 4, 4, 4, 4],\n",
      "          [4, 4, 4, 4, 4],\n",
      "          [4, 4, 4, 4, 4],\n",
      "          [4, 4, 4, 4, 4]]]])\n",
      "tensor([[[[0, 1, 2, 3, 4],\n",
      "          [0, 1, 2, 3, 4],\n",
      "          [0, 1, 2, 3, 4],\n",
      "          [0, 1, 2, 3, 4],\n",
      "          [0, 1, 2, 3, 4]],\n",
      "\n",
      "         [[0, 1, 2, 3, 4],\n",
      "          [0, 1, 2, 3, 4],\n",
      "          [0, 1, 2, 3, 4],\n",
      "          [0, 1, 2, 3, 4],\n",
      "          [0, 1, 2, 3, 4]]],\n",
      "\n",
      "\n",
      "        [[[0, 1, 2, 3, 4],\n",
      "          [0, 1, 2, 3, 4],\n",
      "          [0, 1, 2, 3, 4],\n",
      "          [0, 1, 2, 3, 4],\n",
      "          [0, 1, 2, 3, 4]],\n",
      "\n",
      "         [[0, 1, 2, 3, 4],\n",
      "          [0, 1, 2, 3, 4],\n",
      "          [0, 1, 2, 3, 4],\n",
      "          [0, 1, 2, 3, 4],\n",
      "          [0, 1, 2, 3, 4]]],\n",
      "\n",
      "\n",
      "        [[[0, 1, 2, 3, 4],\n",
      "          [0, 1, 2, 3, 4],\n",
      "          [0, 1, 2, 3, 4],\n",
      "          [0, 1, 2, 3, 4],\n",
      "          [0, 1, 2, 3, 4]],\n",
      "\n",
      "         [[0, 1, 2, 3, 4],\n",
      "          [0, 1, 2, 3, 4],\n",
      "          [0, 1, 2, 3, 4],\n",
      "          [0, 1, 2, 3, 4],\n",
      "          [0, 1, 2, 3, 4]]]])\n",
      "tensor([[[[False, False,  True,  True,  True],\n",
      "          [False, False,  True,  True,  True],\n",
      "          [False, False,  True,  True,  True],\n",
      "          [False, False,  True,  True,  True],\n",
      "          [False, False,  True,  True,  True]],\n",
      "\n",
      "         [[False, False,  True,  True,  True],\n",
      "          [False, False,  True,  True,  True],\n",
      "          [False, False,  True,  True,  True],\n",
      "          [False, False,  True,  True,  True],\n",
      "          [False, False,  True,  True,  True]]],\n",
      "\n",
      "\n",
      "        [[[False, False, False,  True,  True],\n",
      "          [False, False, False,  True,  True],\n",
      "          [False, False, False,  True,  True],\n",
      "          [False, False, False,  True,  True],\n",
      "          [False, False, False,  True,  True]],\n",
      "\n",
      "         [[False, False, False,  True,  True],\n",
      "          [False, False, False,  True,  True],\n",
      "          [False, False, False,  True,  True],\n",
      "          [False, False, False,  True,  True],\n",
      "          [False, False, False,  True,  True]]],\n",
      "\n",
      "\n",
      "        [[[False, False, False, False,  True],\n",
      "          [False, False, False, False,  True],\n",
      "          [False, False, False, False,  True],\n",
      "          [False, False, False, False,  True],\n",
      "          [False, False, False, False,  True]],\n",
      "\n",
      "         [[False, False, False, False,  True],\n",
      "          [False, False, False, False,  True],\n",
      "          [False, False, False, False,  True],\n",
      "          [False, False, False, False,  True],\n",
      "          [False, False, False, False,  True]]]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# mat_valid = valid_lens.repeat(seq_len,1).T\n",
    "mat_valid = valid_lens.repeat(num_heads, seq_len, seq_len, 1).permute(3,0,1,2)\n",
    "print(mat_valid)\n",
    "mat_arrage = torch.arange(seq_len).repeat(batch_size,num_heads, seq_len, 1).to(device)\n",
    "    \n",
    "# mat_arrage = torch.arange(seq_len).repeat(batch_size,1).to(device)\n",
    "print(mat_arrage)\n",
    "mask = mat_arrage>=mat_valid\n",
    "print(mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95441890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[False,  True,  True,  True,  True],\n",
      "          [False, False,  True,  True,  True],\n",
      "          [False, False, False,  True,  True],\n",
      "          [False, False, False, False,  True],\n",
      "          [False, False, False, False, False]],\n",
      "\n",
      "         [[False,  True,  True,  True,  True],\n",
      "          [False, False,  True,  True,  True],\n",
      "          [False, False, False,  True,  True],\n",
      "          [False, False, False, False,  True],\n",
      "          [False, False, False, False, False]]],\n",
      "\n",
      "\n",
      "        [[[False,  True,  True,  True,  True],\n",
      "          [False, False,  True,  True,  True],\n",
      "          [False, False, False,  True,  True],\n",
      "          [False, False, False, False,  True],\n",
      "          [False, False, False, False, False]],\n",
      "\n",
      "         [[False,  True,  True,  True,  True],\n",
      "          [False, False,  True,  True,  True],\n",
      "          [False, False, False,  True,  True],\n",
      "          [False, False, False, False,  True],\n",
      "          [False, False, False, False, False]]],\n",
      "\n",
      "\n",
      "        [[[False,  True,  True,  True,  True],\n",
      "          [False, False,  True,  True,  True],\n",
      "          [False, False, False,  True,  True],\n",
      "          [False, False, False, False,  True],\n",
      "          [False, False, False, False, False]],\n",
      "\n",
      "         [[False,  True,  True,  True,  True],\n",
      "          [False, False,  True,  True,  True],\n",
      "          [False, False, False,  True,  True],\n",
      "          [False, False, False, False,  True],\n",
      "          [False, False, False, False, False]]]])\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones((seq_len, seq_len)), diagonal=1).bool().to(device)\n",
    "mask = mask.repeat(batch_size, num_heads, 1, 1)\n",
    "print(mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e19f677a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "torch.Size([1, 2, 3, 4, 5])\n",
      "0\n",
      "tensor(-0.7141)\n"
     ]
    }
   ],
   "source": [
    "a=torch.randn((2,3,4,5))\n",
    "print(a.dim())\n",
    "print(a.unsqueeze(0).shape)\n",
    "b=torch.randn(())\n",
    "print(b.dim())\n",
    "print(b)\n",
    "print(a.masked_fill(mask, -1e6))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8caf0f28-f078-4a25-9c0a-e1aa5208c5c1",
   "metadata": {},
   "source": [
    "## seq2seq attention 机制中实现attention 函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a84d258-449e-48bd-887a-74f1ed80a111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 针对seq2seq的数据结构，实现注意力机制\n",
    "\n",
    "# # input arg: X, size = [batch_size, seq_len]\n",
    "# # input arg: en_outputs, size = [seq_len, batch_size, dim_hiddens]\n",
    "# # input arg: state, size = [num_layer *  num_direction, batch_size, dim_hiddens]\n",
    "# # input arg: x_vare_len, size = [batch_size]\n",
    "# X = self.emb(X).permute(1, 0, 2) # emb并调整维度顺序后 [seq_len, batch_size, dim_emb]\n",
    "\n",
    "# ### attention\n",
    "# # Q = state; K = en_outputs; V = en_outputs\n",
    "# # res = F.softmax(Q.permute(1,0,2).matmul(K.permute(1,2,0))/math.sqrt(K.shape[-1])).matmul(K.permute(1,0,2))\n",
    "# # dim res = (batch_size, 1, dim_hiddens)\n",
    "# # 主要是维度变换的问题 对于批量矩阵相乘 只要保证前几维度相同即可自动计算后续例如size=(6,5,3,4) 和 size=(6,5,4,1)矩阵相乘得到(6,5,3,1)\n",
    "# atte_scores = state.permute(1,0,2).matmul(en_outputs.permute(1,2,0))\n",
    "# # 计算softmax分数\n",
    "# atte_scores = F.softmax(atte_scores/math.sqrt(en_outputs.shape[-1]), dim=-1)\n",
    "# atten_res = atte_scores.matmul(en_outputs.permute(1,0,2)).squeeze(1)\n",
    "# context = atten_res.repeat(X.shape[0], 1, 1)\n",
    "# # after cat size = [seq_len, batch_size, dim_hiddens + dim_emb]\n",
    "# X_and_context = torch.cat((X, context), 2)\n",
    "# output, state = self.rnn(X_and_context, state)\n",
    "# output = self.dense(output).permute(1, 0, 2)\n",
    "# # output的形状:(batch_size, num_steps, vocab_size)\n",
    "# # state的形状: (num_layers * num_direction, batch_size, num_hiddens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79ea269-1c0f-4145-9a73-9d7b07977a86",
   "metadata": {},
   "source": [
    "### 训练seq2seq attention模型-翻译数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3302ad-aaa3-49c5-aa1c-c39796603efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = './saved_models/nmt_eng2fra_s2s_atten'\n",
    "# batch_size, num_steps = 64, 16\n",
    "# embed_size, num_hiddens, num_layers, dropout = 128, 128, 2, 0.2\n",
    "# incellplot = False\n",
    "# epoch = 100\n",
    "# num_examples = 10000\n",
    "# device = d2l.try_gpu()\n",
    "# model_path = train_s2s_attention_pipline(model_path, batch_size, num_steps,\n",
    "#                            embed_size, num_hiddens, num_layers,\n",
    "#                            dropout,num_epochs=epoch, num_examples=num_examples, incellplot=incellplot)\n",
    "# model_path = './saved_models/nmt_eng2fra_s2s_atten_bz64_ns16_dime128_dimh128_nl2_epo200_lr0.01_1w.pkl'\n",
    "# pridict_s2s_attention_pipline(model_path, batch_size, num_steps, embed_size, num_hiddens, num_layers, dropout, device, num_examples=num_examples)\n",
    "# # 0.110 atten 32 32 10000\n",
    "# # avg blue:0.281, test num:5000 nmt_eng2fra_s2s_atten_bz64_ns16_dime128_dimh128_nl1_epo200_lr0.01_10w.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16ba6095-b137-4722-8045-e31a3dd19a4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T04:25:23.087017Z",
     "iopub.status.busy": "2024-11-07T04:25:23.087017Z",
     "iopub.status.idle": "2024-11-07T04:25:23.106836Z",
     "shell.execute_reply": "2024-11-07T04:25:23.106025Z",
     "shell.execute_reply.started": "2024-11-07T04:25:23.087017Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.7109, -1.8129, -0.3297,  0.3725],\n",
      "         [-1.5190,  1.9149, -0.1870, -1.5384],\n",
      "         [-0.2127, -0.6461, -0.3796, -0.5093],\n",
      "         [ 0.7262, -0.0470,  0.6699,  1.2201]],\n",
      "\n",
      "        [[-0.9459,  0.3858, -0.1374,  0.8444],\n",
      "         [-0.3865,  0.4512, -0.7994,  1.1063],\n",
      "         [-0.7378, -0.8631,  1.7015,  0.8022],\n",
      "         [-0.8016, -1.3035,  2.9029, -0.4328]],\n",
      "\n",
      "        [[ 0.6304,  1.1267, -0.5393, -3.1494],\n",
      "         [-0.1664, -0.3146,  0.1162,  1.6930],\n",
      "         [ 0.2385, -0.2877, -0.2676, -0.3129],\n",
      "         [ 0.0702,  0.6072, -0.0227, -0.7544]],\n",
      "\n",
      "        [[-0.0729, -0.4624,  0.1464,  0.7713],\n",
      "         [ 1.1600,  0.8432, -0.3150, -1.4627],\n",
      "         [ 0.4971, -0.6775,  0.4755,  0.9159],\n",
      "         [-0.0514,  0.8352, -0.5222, -1.2163]],\n",
      "\n",
      "        [[ 0.6708, -0.0872,  1.8572,  0.8870],\n",
      "         [ 0.6597, -0.8832,  0.6984, -0.5014],\n",
      "         [-0.3564, -0.2620, -0.1831, -0.3153],\n",
      "         [-0.4962, -0.8781, -1.5900, -1.6608]]])\n",
      "tensor([[[0.9258, 0.0742, 0.0000, 0.0000],\n",
      "         [0.0313, 0.9687, 0.0000, 0.0000],\n",
      "         [0.6067, 0.3933, 0.0000, 0.0000],\n",
      "         [0.6842, 0.3158, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.2089, 0.7911, 0.0000, 0.0000],\n",
      "         [0.3020, 0.6980, 0.0000, 0.0000],\n",
      "         [0.5313, 0.4687, 0.0000, 0.0000],\n",
      "         [0.6229, 0.3771, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.3784, 0.6216, 0.0000, 0.0000],\n",
      "         [0.5370, 0.4630, 0.0000, 0.0000],\n",
      "         [0.6286, 0.3714, 0.0000, 0.0000],\n",
      "         [0.3689, 0.6311, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.5962, 0.4038, 0.0000, 0.0000],\n",
      "         [0.5785, 0.4215, 0.0000, 0.0000],\n",
      "         [0.7640, 0.2360, 0.0000, 0.0000],\n",
      "         [0.2918, 0.7082, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.6809, 0.3191, 0.0000, 0.0000],\n",
      "         [0.8239, 0.1761, 0.0000, 0.0000],\n",
      "         [0.4764, 0.5236, 0.0000, 0.0000],\n",
      "         [0.5943, 0.4057, 0.0000, 0.0000]]])\n",
      "tensor([[[ 0.8041, -0.6896, -0.6113],\n",
      "         [-0.3112,  0.4191, -1.5525],\n",
      "         [ 0.4062, -0.2940, -0.9470],\n",
      "         [ 0.5029, -0.3901, -0.8654]],\n",
      "\n",
      "        [[ 0.5304,  0.4010, -0.7177],\n",
      "         [ 0.5326,  0.5304, -0.8888],\n",
      "         [ 0.5380,  0.8491, -1.3099],\n",
      "         [ 0.5402,  0.9764, -1.4782]],\n",
      "\n",
      "        [[ 0.1524, -0.9496,  0.1223],\n",
      "         [-0.0894, -0.7032, -0.1843],\n",
      "         [-0.2291, -0.5609, -0.3614],\n",
      "         [ 0.1669, -0.9644,  0.1407]],\n",
      "\n",
      "        [[-0.4058, -0.1744,  0.9846],\n",
      "         [-0.3883, -0.1726,  0.9096],\n",
      "         [-0.5719, -0.1918,  1.6983],\n",
      "         [-0.1045, -0.1428, -0.3100]],\n",
      "\n",
      "        [[-1.1365,  0.5481,  0.6178],\n",
      "         [-0.8658,  0.7243,  0.8900],\n",
      "         [-1.5236,  0.2961,  0.2285],\n",
      "         [-1.3004,  0.4414,  0.4530]]])\n"
     ]
    }
   ],
   "source": [
    "batch_size, num_steps, dim = 5, 4, 3\n",
    "Q = torch.randn((batch_size, num_steps, dim))\n",
    "K = torch.randn((batch_size, num_steps, dim))\n",
    "V = torch.randn((batch_size, num_steps, dim))\n",
    "atten_socre = Q.matmul(K.permute(0,2,1))/math.sqrt(Q.shape[-1])\n",
    "print(atten_socre)\n",
    "atten_socre[:,:,2:]=-1e6\n",
    "atten_socre = F.softmax(atten_socre, dim=-1)\n",
    "outputs = atten_socre.matmul(V)\n",
    "print(atten_socre)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794883e4-ff79-4441-aab9-d0f8d666634f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envdl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
